\chapter{Grammaires formelles et hiérarchie de Chomsky}
\label{grammaires}


\section{Principe général}

Une \textbf{grammaire formelle} est une série de \textbf{règles} permettant de générer des mots. Ces règles utilisent des \textbf{symboles} dits \textbf{terminaux} (les lettres "normales", par convention en minuscules), et d'autres dits \textbf{non-terminaux} (normalement dénotés par des lettres majuscules). Un de ces symboles non-terminaux, appelé \textbf{axiome}, indique le début de toute génération de mot. On présente d'abord le fonctionnement des grammaires et les concepts de base à l'aide de quelques exemples. 

\begin{example}
On présente la grammaire dénotée par ces deux règles :

\[
\begin{cases}
S \rightarrow \epsilon \\
S \rightarrow abS 
\end{cases}
\]

Cette grammaire contient un seul symbole non-terminal, S. Il s'agit donc automatiquement de l'axiome. Le symbole S peut se transformer en $\epsilon$ (première règle) ou en $abS$ (deuxième règle). Une grammaire génère l'ensemble des mots composés uniquement de symboles terminaux (ici $a$ et $b$) qu'on peut obtenir en partant de l'axiome et en appliquant autant de fois qu'on veut les règles données. Dans ce qui suit, on écrira $\rightarrow_1$ pour une application de la première règle et $\rightarrow_2$ pour la seconde.

On peut par exemple obtenir le mot $ab$ de la façon suivante :

\[
S \rightarrow_2 abS \rightarrow_1 ab
\]

Dans cette suite de transformation, appelée \textbf{dérivation}, on remplace d'abord l'axiome par $abS$ à l'aide de la deuxième règle. Puisque $abS$ contient le facteur $S$, on peut utiliser \textit{localement} la deuxième règle pour faire disparaître ce S. Dans ce cas, ce qu'il y avait autour du S, le \textbf{contexte}, reste inchangé.

On peut également obtenir le mot $abab$ :

\[
S \rightarrow_2 \textcolor{blue}{ab}S \rightarrow_2 \textcolor{blue}{ab}\textcolor{red}{ab}S \rightarrow_1 \textcolor{blue}{ab}\textcolor{red}{ab}
\]

ou ababab :


\[
S \rightarrow_2 \textcolor{blue}{ab}S \rightarrow_2 \textcolor{blue}{ab}\textcolor{red}{ab}S \rightarrow_2 \textcolor{blue}{ab}\textcolor{red}{ab}\textcolor{green}{ab}S \rightarrow_1  \textcolor{blue}{ab}\textcolor{red}{ab}\textcolor{green}{ab}
\]

Il n'est pas obligatoire d'utiliser toutes les règles d'une grammaire. On peut donc générer le mot vide :

\[
S \rightarrow_1 \epsilon
\]

On se rend compte assez vite que la grammaire \textbf{engendre} le langage $(ab)^*$.

\end{example}


\begin{example}
\label{LRgram}
Un symbole non-terminal peut générer d'autres symboles non-terminaux, et même plusieurs en même temps. De plus, un non-terminal peut générer un autre mot que juste $\epsilon$. Ces deux points sont illustrés par la grammaire suivante :

\[
\begin{cases}
S \rightarrow LaaR \\
L \rightarrow Lb \\
L \rightarrow ab \\
R \rightarrow bR \\
R \rightarrow ba
\end{cases}
\]

Quelques exemples de dérivations dans cette grammaire :

\[
 S \rightarrow_1 LaaR \rightarrow_2 LbaaR \rightarrow_2 LbbaaR \rightarrow_4 LbbaabR \rightarrow_3 abbbaabR \rightarrow_5 abbbaabba
\]

\[
 S \rightarrow_1 LaaR \rightarrow_3 abaaR \rightarrow_5 abaaba
 \]
 
 On note $\rightarrow_i^j$ $j$ utilisations de la règle numéro $i$, comme dans la dérivation suivante :
 
 \[
 S \rightarrow_1 LaaR \rightarrow_2^5 LbbbbbaaR \rightarrow_3 abbbbbbaaR \rightarrow_4^3 abbbbbbaabbbR \rightarrow_5 abbbbbbaabbbba
 \]

\end{example}

\paragraph{Remarque} Quand on a plusieurs règles de la forme

\[
\begin{cases}
u \rightarrow v_1 \\
... \\
u \rightarrow v_n
\end{cases}
\]

on pourra gagner de la place en écrivant 

\[
\begin{cases}
u \rightarrow v_1~|~...~|~v_n
\end{cases}
\]

Par exemple, la grammaire de l'exemple \ref{LRgram} peut également s'écrire 

\[
\begin{cases}
S \rightarrow LaaR \\
L \rightarrow Lb~|~ab \\
R \rightarrow bR~|~ba 
\end{cases}
\]

Dans ce cas, $L \rightarrow Lb$ est la deuxième règle de la grammaire, tandis que $L \rightarrow ab$ en est la troisième.


\begin{exercice}
Quel est le langage engendré par la grammaire de l'exemple \ref{LRgram} ?
\end{exercice}

\begin{exercice}
\label{grammab}
Quel est le langage engendré par la grammaire suivante ?

\[
\begin{cases}
S \rightarrow A~|~B \\
A \rightarrow aA~|~\epsilon \\
B \rightarrow bB~|~\epsilon
\end{cases}
\]

\end{exercice}

\begin{exercice}
\label{grammsigma}
Quel est le langage engendré par la grammaire suivante ?

\[
\begin{cases}
S \rightarrow aS~|~bS~|~\epsilon 
\end{cases}
\]

\end{exercice}

\begin{exercice}
Donner l'ensemble des mots qui admettent deux dérivations (ie. peuvent être construits de plusieurs façons différentes) dans la grammaire de l'exercice \ref{grammab}. Même question pour celle de l'exercice \ref{grammsigma}.
\end{exercice}

\begin{example}
\label{ggramabcn}
On a jusqu'ici seulement vu des exemples de grammaires avec un seul non-terminal à gauche des flèches de réécriture. Il est cependant possible de préciser le contexte dans lequel les réécritures doivent se faire, comme dans la grammaire suivante :

\[
\begin{cases}
S \rightarrow SABC~|~\epsilon \\
AB \rightarrow BA \\
BA \rightarrow AB \\
AC \rightarrow CA \\
CA \rightarrow AC \\
BC \rightarrow CB \\
CB \rightarrow BC \\
A \rightarrow a \\
B \rightarrow b \\
C \rightarrow c
\end{cases}
\]

Toute dérivation dans cette grammaire fonctionne de la façon suivante : on commence par utiliser la première règle $n$ fois pour produire $S(ABC)^n$. Ensuite, on utilise la deuxième règle pour faire disparaître S et se retrouver avec $(ABC)^n$. Les règles 3 à 8 permettent ensuite de mélanger les symboles non-terminaux comme on l'entend. Une fois qu'ils ont été disposés de la façon voulue, on les transforme en $a$, $b$ et $c$ avec les règles 9, 10 et 11.

Le langage engendré par cette grammaire est donc celui des mots contenant autant de $a$ que de $b$ et de $c$.
\end{example}

\paragraph{Remarque} Dans l'exemple ci-dessus, on découpe toute dérivation en 4 étapes : utilisation de la première règle, puis de la seconde, puis des 3 à 8, et enfin des 9 à 11. Cette présentation nous semble améliorer la compréhension de l'exemple et du langage engendré, mais techniquement, rien n'empêche de mélanger les étapes, comme dans la dérivation suivante :

 \[
 S \rightarrow_1 SABC \rightarrow_9 SaBC \rightarrow_7 SaCB \rightarrow_{11} SaCb \rightarrow_1 SABCaCb \rightarrow_2 ABCaCB \rightarrow_{9,10,11}^5 abcacb 
 \]

 Où $\rightarrow_{9,10,11}^5$ indique 5 utilisations de règles parmi 9, 10 et 11.
 
 \section{Formalisation}
 
 Maintenant qu'on a un peu joué avec les grammaires formelles, on peut regarder comment elles se formalisent. Techniquement donc, une grammaire formelle est un quadruplet $\big \langle\Sigma, V, S, R \big \rangle$, où
 
 \begin{itemize}
 \item $\Sigma$ est l'ensemble des \textbf{symboles terminaux}
 \item $V$ est l'ensemble des \textbf{symboles non-terminaux}
 \item $S \in V$ est l'\textbf{axiome}
 \item $R$ est l'ensemble des \textbf{règles de production}, ou règles de réécriture. Ces dernières forment un sous-ensemble de $(\Sigma \cup V)^*V(\Sigma \cup V)^* \times (\Sigma \cup V)^*$, cad. qu'elles doivent toujours avoir au moins un non-terminal à gauche. 
 \end{itemize}
 
 
 
\begin{example}
La grammaire de l'exemple \ref{LRgram} s'écrit 

\[
\big \langle \{a,b\},\{S,L,R\},S, \begin{cases}
S \rightarrow LaaR \\
L \rightarrow Lb~|~ab \\
R \rightarrow bR~|~ba
\end{cases}
 \big \rangle
\]


\begin{definition}{Réécriture / dérivation immédiate}{}
Soient $p$, $s$ et $g$ des mots sur l'alphabet $(\Sigma \cup V)$ (ils sont donc potentiellement vides), et $f \in (\Sigma \cup V)^*V(\Sigma \cup V)^*$. Si la règle $r$ est de la forme $f \rightarrow g$, alors le mot $pfs$ se \textbf{réécrit} (ou dérive) \textbf{immédiatement} en $pgs$ via la règle $r$, ce qu'on note 

\[
pfs \rightarrow_r pgs
\]

Dit autrement, si le mot $f$ apparaît dans le contexte $p \_ s$, alors on peut transformer $f$ en $g$ en gardant le contexte.

Plus généralement, étant donnée une grammaire $G$, on dit qu'elle réécrit (ou dérive) immédiatement un mot $u$ en $v$ ssi. il existe une règle $r$ dans la grammaire telle que $u$ se réécrit immédiatement en $v$ via la règle $r$. On écrit ceci 

\[
pfs \rightarrow_G pgs
\]

Quand il n'y a pas d'ambiguïté sur la grammaire étudiée, on pourra se permettre de ne pas noter le $G$ et écrire 

\[
pfs \rightarrow pgs
\]

\end{definition}

\end{example}


\begin{definition}{Réécriture / dérivation}{}
Soient une grammaire $G$ et deux mots $u$ et $v$. On dit que $u$ se \textbf{réécrit} (ou dérive) en $v$ avec $G$ ssi. il existe une série (potentiellement nulle) de réécritures immédiates menant de $u$ à $v$ dans $G$. On le note 

\[
u \rightarrow_G^* v
\]

ou $u \rightarrow^* v$ s'il n'y a pas d'ambiguïté sur la grammaire étudiée.

\end{definition}
 
\begin{example}
Dans l'exemple \ref{ggramabcn}, on a 

\[
S \rightarrow^* SABCaCb \rightarrow^* abcacb
\]

et donc 

\[
S \rightarrow^* abcacb
\]
\end{example}
 
%\begin{definition}{Proto-mot}{}
%Un \textbf{proto-mot} est un mot contenant 
%\end{definition}
% proto-mot, dérivations équivalentes, gauche
% defs utiles dans la suite ?

\begin{definition}{Langage engendré}{}
Le langage engendré par le mot $f$ dans une grammaire $G$, noté $L_G(f)$, est l'ensemble des mots de $\Sigma^*$ (formés uniquement de symboles terminaux) en lesquelles on peut réécrire l'axiome de $f$ en suivant les règles de $G$. Plus formellement,

\[
L_G(f) = \{u \in \Sigma^* ~|~f \rightarrow_G^* u\}
\]

Le langage engendré par une grammaire est le langage engendré par l'axiome dans cette grammaire. Soit une grammaire $G$ d'axiome $S$, son langage engendré, noté $L_G$, est $L_G(S)$.
\end{definition}

\begin{example}
Soit $G$ la grammaire de l'exemple \ref{ggramabcn}. 

\[
L_G = \{w \in \Sigma^* ~|~|w|_a = |w|_b = |w|_c \}
\]
\end{example}


\begin{exercice}
Donner une grammaire qui engendre le langage $\{a^nb^n ~|~ n \in \mathbb{N}\}$
\end{exercice}

\begin{exercice}
\label{grammanbncn}
Montrer que la grammaire suivante engendre le langage $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$

\[
\begin{cases}
S \rightarrow XY \\
X \rightarrow aXbZ~|~\epsilon\\
Zb \rightarrow bZ \\
ZY \rightarrow Yc \\
Y \rightarrow \epsilon
\end{cases}
\]
\end{exercice}

\section{Hierarchie de Chomsky}

Chomsky propose en 1956 une classification des grammaires en fonction des formes de leurs règles. Les grammaires sont donc divisées en 4 catégories, appelées grammaires de type 0, 1, 2 et 3.

\subsection{Grammaires de type 3, ou régulières}

Les grammaires de type 3, appelées \textbf{grammaires régulières}, se divisent elles-même en deux sous-classes : les \textbf{grammaires linéaires à gauche} et \textbf{grammaires linéaires à droite}.

\subsubsection{Grammaires linéaires à gauche et à droite}

Dans les deux cas, on ne permet que d'écrire un symbole terminal à la fois, en allant toujours dans le même sens.

\begin{definition}{Grammaire linéaire à gauche}{}
Une grammaire linéaire à gauche est une grammaire dont toutes les règles sont de la forme 
\begin{itemize}
\item[] $A \rightarrow Ba$
\item[] ou
\item[] $A \rightarrow a$
\item[] ou
\item[] $A \rightarrow \epsilon$
\end{itemize}

où $A$ et $B$ sont des symboles non-terminaux, et $a$ un terminal.
\end{definition}

\begin{example}
\label{gramling}
La grammaire 
\[
\begin{cases}
S \rightarrow Sa~|~Ta~|~a \\
T \rightarrow Tb~|~b
\end{cases}
\]
est une grammaire linéaire à gauche. 
\end{example}

\begin{exercice}
Quel est le langage engendré par la grammaire de l'exemple \ref{gramling} ?
\end{exercice}


\begin{definition}{Grammaire linéaire à droite}{}
Une grammaire linéaire à droite est une grammaire dont toutes les règles sont de la forme 
\begin{itemize}
\item[] $A \rightarrow aB$
\item[] ou
\item[] $A \rightarrow a$
\item[] ou
\item[] $A \rightarrow \epsilon$
\end{itemize}

où $A$ et $B$ sont des symboles non-terminaux, et $a$ un terminal.
\end{definition}

\begin{example}
\label{gramlind}
La grammaire 
\[
\begin{cases}
S \rightarrow aS~|~bS~|~\epsilon
\end{cases}
\]
est une grammaire linéaire à droite. 
\end{example}


\begin{exercice}
Quel est le langage engendré par la grammaire de l'exemple \ref{gramlind} ?
\end{exercice}

\paragraph{Remarque} Les grammaires de type 3, ou régulières, est l'ensemble des grammaires linéaires à gauche et des grammaires linéaires à droite, mais sans mélange. Par exemple, la grammaire 
\[
\begin{cases}
S \rightarrow aS~|~Sb~|~\epsilon
\end{cases}
\]
\textbf{n}'est \textbf{pas} une grammaire de type 3. 

\begin{lemma}
Les grammaires linéaires à gauche et les grammaires linéaire à droite ont la même expressivité. Dit autrement, tout langage engendré par une grammaire linéaire à gauche est est également engendré par une grammaire linéaire à droite, et inversement.
\end{lemma}

\begin{proof}
On n'entrera pas ici dans les détails, qui dépassent le cadre de ce cours. Etant donnée une grammaire linéaire à gauche (resp. droite), on peut écrire une grammaire linéaire à droite (resp. gauche) qui la simule en partant des règles $A \rightarrow a$ et $A \rightarrow \epsilon$. Les règles \textit{normales} sont inversées et nouvelles règles \textit{terminales} correspondent à l'axiome de la première grammaire. 
\end{proof}

\begin{exercice}(**) Donner, en suivant l'intuition de la preuve ci-dessus, une grammaire linéaire à droite qui engendre le même langage que la grammaire de l'exemple \ref{gramling}.
\end{exercice}

\subsubsection{Une extension du théorème de Kleene}
On a vu dans le chapitre \ref{hierarchie} que les langages pouvant être décrits par des expressions rationnelles et ceux définissables par automate étaient les mêmes. Ce résultat peut s'étendre aux grammaires de type 3.

\begin{theorem}
\label{kleeneplusplus}
Les langages reconnus par automate fini sont exactement ceux engendrés par des grammaires de type 3.
\end{theorem}

\begin{proof}
Comme dans le théorème de Kleene original, on propose comme preuve une tradition des grammaires vers les automates, et inversement. Contrairement aux expressions rationnelles, les grammaires de type 3 ont un fonctionnement très similaire aux automates finis, les traductions sont donc très simples.

Soit $G$ une grammaire de type 3. Puisque les grammaires linéaires à gauche et à droite engendrent les mêmes langages, on peut supposer que $G$ est une grammaire linéaire à droite. On crée un automate dont les états sont les symboles non-terminaux de $G$, ainsi qu'un état supplémentaire et terminal $T$ (pour terminus). L'état initial est son axiome. Toute règle $A \rightarrow aB$ devient une transition de $A$ vers $B$ en lisant $a$. Les règles $A \rightarrow a$ deviennent des transitions de $A$ vers $T$ en $a$. Enfin, on rend terminal tout état $A$ tel qu'il existe une règle $A \rightarrow \epsilon$.

Soit $A$ un automate fini. On déterminise $A$ en $A_{det}$. On crée une grammaire dont les symboles non-terminaux sont les états de $A_{det}$. L'axiome est l'état initial de $A_{det}$. Toute transition $A \xrightarrow{a} B$ se traduit par une règle $A \rightarrow aB $. Pour tout état terminal $A$, on ajoute une règle $A \rightarrow \epsilon$.
\end{proof}

\begin{example}
Soit la grammaire suivante :
\[
\begin{cases}
S \rightarrow aS~|~aA~|~bA~|~a \\
A \rightarrow aB~|~b~|~\epsilon \\
B \rightarrow b~|~aS~|~bS 
\end{cases}
\]

Le langage qu'elle engendre est reconnu par l'automate suivant :


\begin{figure}[!ht]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.9cm,
                    semithick]
  \tikzstyle{every state}=[fill=white,text=black]
  \tikzstyle{place}=[rectangle,draw=black,fill=white, minimum size=7mm]


  \node[initial,state] (S)                    {$S$};
  \node[state,accepting] (A)      [right of=S]                {$A$};  
  \node[state] (B)      [right of=A]                {$B$};   
  \node[state,accepting] (T)      [above of=A]                {$T$};    

  \path %(I) edge[loop above]              node {$a,b$} (I)
(S) edge      [loop above]        node {$a$} (S)
(S) edge      []        node {$a,b$} (A)
(S) edge      []        node {$a$} (T)
(A) edge      []        node {$b$} (T)
(A) edge      []        node {$a$} (B)
(B) edge      []        node {$b$} (T)
(B) edge      [bend left]        node {$a,b$} (S);
\end{tikzpicture}
\end{figure}

\end{example}

\begin{example}
L'automate de l'exemple \ref{autopasaba} est simulé par la grammaire qui suit, où $0$ est remplacé par $A$, $1$ par $B$ etc, et $A$ est l'axiome.

\[
\begin{cases}
A \rightarrow bA~|~aB \\
B \rightarrow aB~|~bC \\
C \rightarrow aD~|~bA \\
D \rightarrow aD~|~bD
\end{cases}
\]

\end{example}

\begin{exercice}
Traduire en automate la grammaire 
\[
\begin{cases}
S \rightarrow aS~|~bB~|~a~|~\epsilon \\
A \rightarrow aS~|~\epsilon \\
B \rightarrow a
\end{cases}
\]

\end{exercice}

\begin{exercice}
Traduire en grammaire l'automate de l'exemple \ref{detex1}.
\end{exercice}



\subsection{Grammaires de type 2, ou non contextuelles}

Les grammaires de type 2, appelées \textbf{grammaires non contextuelles} (\textit{context-free}), ou parfois grammaires algébriques, n'ont aucune contrainte quant à la partie droite des règles. Elles imposent cependant que la partie gauche ne contienne pas plus d'un caractère, empêchant donc de prendre en compte le contexte dans les dérivations.

\begin{definition}{Grammaire non contextuelle}{}
Une grammaire non contextuelle est une grammaire dont toutes les règles sont de la forme 
\begin{itemize}
\item[] $A \rightarrow \gamma$
\end{itemize}
où $A$ est un symbole non-terminal, et $\gamma$ est n'importe quel mot constitué de symboles terminaux et non-terminaux.
\end{definition}

\begin{example}
\label{gramanbn}
La grammaire 

\[
\begin{cases}
S \rightarrow aSb \\
S \rightarrow \epsilon
\end{cases}
\]

est une grammaire de type 2, qui engendre le langage $\{a^nb^n ~|~n \in \mathbb{N}\}$
\end{example}

\paragraph{Remarque} Toute grammaire de type 3 est également une grammaire de type 2, puisque toute règle d'une grammaire de type 3 a bien un seul symbole à gauche de toute règle.

\begin{lemma}
Les langages engendrés par des grammaires de type 3 forment un sous-ensemble strict des langages engendrés par des grammaires de type 2. Autrement dit, si un langage est engendré par une grammaire de type 3, alors il l'est également par une grammaire de type 2, mais il existe au moins un (et même plein) langage engendré par une grammaire de type 2 qui ne peut pas être engendré par une grammaire de type 3.  
\end{lemma}

\begin{proof}
L'inclusion non-stricte est triviale avec la remarque précédente. En effet, soit $L$ un langage engendré par la grammaire de type 3 $G$, alors $G$ est également une gramaire de type 2 qui, de fait, engendre toujours $L$.

Pour l'aspect strict de cette inclusion, la grammaire de type 2 de l'exemple \ref{gramanbn} engendre le langage $\{a^nb^n ~|~ n \in \mathbb{N}\}$. Or, on a vu en \ref{limiterecauto} que ce langage n'était pas reconnaissable par automate. Puisque les grammaires de type 3 ont la même expressivité que ces derniers (théorème \ref{kleeneplusplus}), il n'en existe pas qui engendre ce langage.
\end{proof}




\subsection{Grammaires de type 1, ou contextuelles}

Les grammaires de type 1, appelées \textbf{grammaires contextuelles} (\textit{context-sensitive}) permettent de mettre n'importe quel mot (contenant au moins un symbole non-terminal) à gauche d'une flèche. La seule contrainte est désormais qu'on ne permet pas d'avoir le mot vide à droite d'une flèche, sauf cas particulier pour capturer le mot vide.

\begin{definition}{Grammaire contextuelle}{}
Une grammaire contextuelle est une grammaire dont toutes les règles sont de la forme 
\begin{itemize}
\item[] $\alpha \rightarrow \beta$, où $\alpha$ et $\beta$ sont des mots constitué de symboles terminaux et non-terminaux, tels que $\alpha$ contient au moins un symbole non-terminal et $\beta \neq \epsilon$
\item[] ou
\item[] $S \rightarrow \epsilon$, où $S$ est l'axiome de la grammaire. Si cette règle est utilisée, $S$ n'a pas le droit d'apparaître à droite d'une règle.
\end{itemize}

\end{definition}

Cette exception un peu étrange permet aux grammaires contextuelles d'engendrer le mot vide, ce qui ne serait pas possible uniquement avec le premier type de règle. Supposons par exemple qu'on ait une grammaire contextuelle $G$ d'axiome $S$ engendrant un langage $L$ ne contenant pas le mot vide, on peut facilement construire la grammaire $G'$ qui engendre $L \cup \{\epsilon\}$. 

Il suffit d'ajouter un nouveau non-terminal, $S'$, qui sera le nouvel axiome, ainsi que les deux règles qui suivent : 


\[
\begin{cases}
S' \rightarrow \epsilon \\
S' \rightarrow S
\end{cases}
\]

Ainsi, on permet d'engendrer $\epsilon$ dès le début, ou de faire une génération "normale" de $G$ en repartant de $S$. Avec cette transformation, on est sûr que $S'$ n'apparaît jamais à droite d'une règle.

\begin{example}
\label{grammanbncnbis}
La grammaire de l'exercice \ref{grammanbncn} \textbf{n}'est \textbf{pas} une grammaire de type 1, à cause des règles $X \rightarrow \epsilon$ et $Y \rightarrow \epsilon$. On peut cependant légèrement la modifier pour obtenir une grammaire de type 1 qui engendre le même langage, c'est-à-dire $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$


\[
\begin{cases}
S \rightarrow XY~|~\epsilon \\
X \rightarrow aXbZ~|~abZ\\
Zb \rightarrow bZ \\
ZY \rightarrow Yc \\
bY \rightarrow b
\end{cases}
\]

\end{example}

\begin{exercice}
Montrer que la grammaire de l'exemple \ref{grammanbncnbis} engendre bien le langage \newline $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$.
\end{exercice}

\paragraph{Remarque} Une grammaire de type 2 \textbf{n}'est \textbf{pas forcément} une grammaire de type 1, à cause des règles de la forme $A \rightarrow \epsilon$. Il existe cependant un algorithme qui prend une grammaire de type 2 et en renvoie une autre, également de type 2, engendrant le même langage sans règle de la forme $A \rightarrow \epsilon$, sauf éventuellement pour l'axiome. Cet algorithme sera décrit en ???. %TODO


\begin{lemma}
Les langages engendrés par des grammaires de type 2 forment un sous-ensemble strict des langages engendrés par des grammaires de type 1. Autrement dit, si un langage est engendré par une grammaire de type 2, alors il l'est également par une grammaire de type 1, mais il existe au moins un (et même plein) langage engendré par une grammaire de type 1 qui ne peut pas être engendré par une grammaire de type 2.  
\end{lemma}

\begin{proof}
L'inclusion non-stricte est encore une fois simple avec la remarque précédente. En effet, soit $L$ un langage engendré par la grammaire de type 2 $G$, alors on peut construire $G'$ une gramaire de type 2 qui est également de type 1 et engendre toujours $L$.

Pour l'aspect strict de cette inclusion, la grammaire de type 1 de l'exemple \ref{grammanbncnbis} engendre le langage $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$. On n'en donnera malheureusement pas la preuve ici, mais ce langage n'est pas engendrable par une grammaire de type 2\footnote{Vous pouvez cependant essayer de vous en convaincre en essayant d'écrire une telle grammaire !}.
\end{proof}






\subsection{Grammaires de type 0, ou générales}

Les grammaires de type 0, appelées \textbf{grammaires générales}, sont tout simplement l'ensemble des grammaires formelles. Elles n'ont donc aucune contrainte, sinon toujours celle d'avoir au moins un symbole non-terminal à gauche de toute règle.
Il va sans dire que toute grammaire présenté jusqu'ici ou dans la suite de ce document constitue une grammaire de type $0$. 

\begin{lemma}
Les langages engendrés par des grammaires de type 1 forment un sous-ensemble strict des langages engendrés par des grammaires de type 0. Autrement dit, si un langage est engendré par une grammaire de type 1, alors il l'est également par une grammaire de type 0, mais il existe au moins un (et même plein) langage engendré par une grammaire de type 0 qui ne peut pas être engendré par une grammaire de type 1.  
\end{lemma}

\begin{proof}
L'inclusion non-stricte est triviale, puisque toute grammaire de type 1 est également une grammaire de type 0.

Pour l'aspect strict de cette inclusion, c'est un peu plus compliqué, puisqu'il n'existe pas - à notre connaissance - d'exemple très concret et naturel de langage strictement engendrable par une grammaire de type 0. On renverra par contre à \href{https://cs.stackexchange.com/a/56634}{cet exemple abstrait et très amusant}\footnote{https://cs.stackexchange.com/a/56634}. Informellement, l'idée est de faire une liste (infinie) des mots sur un alphabet, ainsi qu'une liste (également infinie) des grammaires de type 1 sur le même alphabet (c'est un peu technique, mais on peut écrire un algorithme qui parcourt cet ensemble sans rien oublier). 

On aligne ces deux listes, cad. qu'on a maintenant une liste de paires (un mot et une grammaires, associés arbitrairement). On définit maintenant $L$ le langage des mots qui n'appartiennent pas au langage engendré par la grammaire qui leur est associée. Sans entrer dans les détails, ce langage est bien engendré par une grammaire de type 0 consistant en gros en un "simulateur de grammaire de type 1".

Il n'est par contre pas engendré par une grammaire de type $1$. En effet, si une telle grammaire $G$ existait, elle serait à une position $j$ de la liste des grammaires. Dans ce cas, le $j^{eme}$ mot appartient à $L$ ssi. il n'appartient pas à $L_G$. La grammaire $G$ ne peut donc en fait pas engendrer $L$.
\end{proof}