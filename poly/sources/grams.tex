\chapter{Grammaires formelles et hiérarchie de Chomsky}
\label{grammaires}


\section{Principe général}

Une \textbf{grammaire formelle} est une série de \textbf{règles} permettant de générer des mots. Ces règles utilisent des \textbf{symboles} dits \textbf{terminaux} (les lettres "normales", par convention en minuscules), et d'autres dits \textbf{non-terminaux} (normalement dénotés par des lettres majuscules). Un de ces symboles non-terminaux, appelé \textbf{axiome}, indique le début de toute génération de mot. On présente d'abord le fonctionnement des grammaires et les concepts de base à l'aide de quelques exemples. 

\begin{example}
On présente la grammaire dénotée par ces deux règles :

\[
\begin{cases}
S \rightarrow \epsilon \\
S \rightarrow abS 
\end{cases}
\]

Cette grammaire contient un seul symbole non-terminal, S. Il s'agit donc automatiquement de l'axiome. Le symbole S peut se transformer en $\epsilon$ (première règle) ou en $abS$ (deuxième règle). Une grammaire génère l'ensemble des mots composés uniquement de symboles terminaux (ici $a$ et $b$) qu'on peut obtenir en partant de l'axiome et en appliquant autant de fois qu'on veut les règles données. Dans ce qui suit, on écrira $\rightarrow_1$ pour une application de la première règle et $\rightarrow_2$ pour la seconde.

On peut par exemple obtenir le mot $ab$ de la façon suivante :

\[
S \rightarrow_2 abS \rightarrow_1 ab
\]

Dans cette suite de transformation, appelée \textbf{dérivation}, on remplace d'abord l'axiome par $abS$ à l'aide de la deuxième règle. Puisque $abS$ contient le facteur $S$, on peut utiliser \textit{localement} la deuxième règle pour faire disparaître ce S. Dans ce cas, ce qu'il y avait autour du S, le \textbf{contexte}, reste inchangé.

On peut également obtenir le mot $abab$ :

\[
S \rightarrow_2 \textcolor{blue}{ab}S \rightarrow_2 \textcolor{blue}{ab}\textcolor{red}{ab}S \rightarrow_1 \textcolor{blue}{ab}\textcolor{red}{ab}
\]

ou ababab :


\[
S \rightarrow_2 \textcolor{blue}{ab}S \rightarrow_2 \textcolor{blue}{ab}\textcolor{red}{ab}S \rightarrow_2 \textcolor{blue}{ab}\textcolor{red}{ab}\textcolor{green}{ab}S \rightarrow_1  \textcolor{blue}{ab}\textcolor{red}{ab}\textcolor{green}{ab}
\]

Il n'est pas obligatoire d'utiliser toutes les règles d'une grammaire. On peut donc générer le mot vide :

\[
S \rightarrow_1 \epsilon
\]

On se rend compte assez vite que la grammaire \textbf{engendre} le langage $(ab)^*$.

\end{example}


\begin{example}
\label{LRgram}
Un symbole non-terminal peut générer d'autres symboles non-terminaux, et même plusieurs en même temps. De plus, un non-terminal peut générer un autre mot que juste $\epsilon$. Ces deux points sont illustrés par la grammaire suivante :

\[
\begin{cases}
S \rightarrow LaaR \\
L \rightarrow Lb \\
L \rightarrow ab \\
R \rightarrow bR \\
R \rightarrow ba
\end{cases}
\]

Quelques exemples de dérivations dans cette grammaire :

\[
 S \rightarrow_1 LaaR \rightarrow_2 LbaaR \rightarrow_2 LbbaaR \rightarrow_4 LbbaabR \rightarrow_3 abbbaabR \rightarrow_5 abbbaabba
\]

\[
 S \rightarrow_1 LaaR \rightarrow_3 abaaR \rightarrow_5 abaaba
 \]
 
 On note $\rightarrow_i^j$ $j$ utilisations de la règle numéro $i$, comme dans la dérivation suivante :
 
 \[
 S \rightarrow_1 LaaR \rightarrow_2^5 LbbbbbaaR \rightarrow_3 abbbbbbaaR \rightarrow_4^3 abbbbbbaabbbR \rightarrow_5 abbbbbbaabbbba
 \]

\end{example}

\paragraph{Remarque} Quand on a plusieurs règles de la forme

\[
\begin{cases}
u \rightarrow v_1 \\
... \\
u \rightarrow v_n
\end{cases}
\]

on pourra gagner de la place en écrivant 

\[
\begin{cases}
u \rightarrow v_1~|~...~|~v_n
\end{cases}
\]

Par exemple, la grammaire de l'exemple \ref{LRgram} peut également s'écrire 

\[
\begin{cases}
S \rightarrow LaaR \\
L \rightarrow Lb~|~ab \\
R \rightarrow bR~|~ba 
\end{cases}
\]

Dans ce cas, $L \rightarrow Lb$ est la deuxième règle de la grammaire, tandis que $L \rightarrow ab$ en est la troisième.


\begin{exercice}
Quel est le langage engendré par la grammaire de l'exemple \ref{LRgram} ?
\end{exercice}

\begin{exercice}
\label{grammab}
Quel est le langage engendré par la grammaire suivante ?

\[
\begin{cases}
S \rightarrow A~|~B \\
A \rightarrow aA~|~\epsilon \\
B \rightarrow bB~|~\epsilon
\end{cases}
\]

\end{exercice}

\begin{exercice}
\label{grammsigma}
Quel est le langage engendré par la grammaire suivante ?

\[
\begin{cases}
S \rightarrow aS~|~bS~|~\epsilon 
\end{cases}
\]

\end{exercice}

\begin{exercice}
Donner l'ensemble des mots qui admettent deux dérivations (ie. peuvent être construits de plusieurs façons différentes) dans la grammaire de l'exercice \ref{grammab}. Même question pour celle de l'exercice \ref{grammsigma}.
\end{exercice}

\begin{example}
\label{ggramabcn}
On a jusqu'ici seulement vu des exemples de grammaires avec un seul non-terminal à gauche des flèches de réécriture. Il est cependant possible de préciser le contexte dans lequel les réécritures doivent se faire, comme dans la grammaire suivante :

\[
\begin{cases}
S \rightarrow SABC~|~\epsilon \\
AB \rightarrow BA \\
BA \rightarrow AB \\
AC \rightarrow CA \\
CA \rightarrow AC \\
BC \rightarrow CB \\
CB \rightarrow BC \\
A \rightarrow a \\
B \rightarrow b \\
C \rightarrow c
\end{cases}
\]

Toute dérivation dans cette grammaire fonctionne de la façon suivante : on commence par utiliser la première règle $n$ fois pour produire $S(ABC)^n$. Ensuite, on utilise la deuxième règle pour faire disparaître S et se retrouver avec $(ABC)^n$. Les règles 3 à 8 permettent ensuite de mélanger les symboles non-terminaux comme on l'entend. Une fois qu'ils ont été disposés de la façon voulue, on les transforme en $a$, $b$ et $c$ avec les règles 9, 10 et 11.

Le langage engendré par cette grammaire est donc celui des mots contenant autant de $a$ que de $b$ et de $c$.
\end{example}

\paragraph{Remarque} Dans l'exemple ci-dessus, on découpe toute dérivation en 4 étapes : utilisation de la première règle, puis de la seconde, puis des 3 à 8, et enfin des 9 à 11. Cette présentation nous semble améliorer la compréhension de l'exemple et du langage engendré, mais techniquement, rien n'empêche de mélanger les étapes, comme dans la dérivation suivante :

 \[
 S \rightarrow_1 SABC \rightarrow_9 SaBC \rightarrow_7 SaCB \rightarrow_{11} SaCb \rightarrow_1 SABCaCb \rightarrow_2 ABCaCB \rightarrow_{9,10,11}^5 abcacb 
 \]

 Où $\rightarrow_{9,10,11}^5$ indique 5 utilisations de règles parmi 9, 10 et 11.
 
 \section{Formalisation}
 
 Maintenant qu'on a un peu joué avec les grammaires formelles, on peut regarder comment elles se formalisent. Techniquement donc, une grammaire formelle est un quadruplet $\big \langle\Sigma, V, S, R \big \rangle$, où
 
 \begin{itemize}
 \item $\Sigma$ est l'ensemble des \textbf{symboles terminaux}
 \item $V$ est l'ensemble des \textbf{symboles non-terminaux}
 \item $S \in V$ est l'\textbf{axiome}
 \item $R$ est l'ensemble des \textbf{règles de production}, ou règles de réécriture. Ces dernières forment un sous-ensemble de $(\Sigma \cup V)^*V(\Sigma \cup V)^* \times (\Sigma \cup V)^*$, cad. qu'elles doivent toujours avoir au moins un non-terminal à gauche. 
 \end{itemize}
 
 
 
\begin{example}
\label{grammLR}
La grammaire de l'exemple \ref{LRgram} s'écrit 

\[
\big \langle \{a,b\},\{S,L,R\},S, \begin{cases}
S \rightarrow LaaR \\
L \rightarrow Lb~|~ab \\
R \rightarrow bR~|~ba
\end{cases}
 \big \rangle
\]


\begin{definition}{Réécriture / dérivation immédiate}{}
Soient $p$, $s$ et $g$ des mots sur l'alphabet $(\Sigma \cup V)$ (ils sont donc potentiellement vides), et $f \in (\Sigma \cup V)^*V(\Sigma \cup V)^*$. Si la règle $r$ est de la forme $f \rightarrow g$, alors le mot $pfs$ se \textbf{réécrit} (ou dérive) \textbf{immédiatement} en $pgs$ via la règle $r$, ce qu'on note 

\[
pfs \rightarrow_r pgs
\]

Dit autrement, si le mot $f$ apparaît dans le contexte $p \_ s$, alors on peut transformer $f$ en $g$ en gardant le contexte.

Plus généralement, étant donnée une grammaire $G$, on dit qu'elle réécrit (ou dérive) immédiatement un mot $u$ en $v$ ssi. il existe une règle $r$ dans la grammaire telle que $u$ se réécrit immédiatement en $v$ via la règle $r$. On écrit ceci 

\[
pfs \rightarrow_G pgs
\]

Quand il n'y a pas d'ambiguïté sur la grammaire étudiée, on pourra se permettre de ne pas noter le $G$ et écrire 

\[
pfs \rightarrow pgs
\]

\end{definition}

\end{example}


\begin{definition}{Réécriture / dérivation}{}
Soient une grammaire $G$ et deux mots $u$ et $v$. On dit que $u$ se \textbf{réécrit} (ou dérive) en $v$ avec $G$ ssi. il existe une série (potentiellement nulle) de réécritures immédiates menant de $u$ à $v$ dans $G$. On le note 

\[
u \rightarrow_G^* v
\]

ou $u \rightarrow^* v$ s'il n'y a pas d'ambiguïté sur la grammaire étudiée.

Pour une série d'au moins une dérivation, on écrira $u \rightarrow_G^+ v$ ou $u \rightarrow^+ v$
\end{definition}
 
\begin{example}
Dans l'exemple \ref{ggramabcn}, on a 

\[
S \rightarrow^* SABCaCb \rightarrow^* abcacb
\]

et donc 

\[
S \rightarrow^* abcacb
\]
\end{example}
 
%\begin{definition}{Proto-mot}{}
%Un \textbf{proto-mot} est un mot contenant 
%\end{definition}
% proto-mot, dérivations équivalentes, gauche
% defs utiles dans la suite ?

\begin{definition}{Langage engendré}{}
Le langage engendré par le mot $f$ dans une grammaire $G$, noté $L_G(f)$, est l'ensemble des mots de $\Sigma^*$ (formés uniquement de symboles terminaux) en lesquelles on peut réécrire l'axiome de $f$ en suivant les règles de $G$. Plus formellement,

\[
L_G(f) = \{u \in \Sigma^* ~|~f \rightarrow_G^* u\}
\]

Le langage engendré par une grammaire est le langage engendré par l'axiome dans cette grammaire. Soit une grammaire $G$ d'axiome $S$, son langage engendré, noté $L_G$, est $L_G(S)$.
\end{definition}

\begin{example}
Soit $G$ la grammaire de l'exemple \ref{ggramabcn}. 

\[
L_G = \{w \in \Sigma^* ~|~|w|_a = |w|_b = |w|_c \}
\]
\end{example}


\begin{exercice}
Donner une grammaire qui engendre le langage $\{a^nb^n ~|~ n \in \mathbb{N}\}$
\end{exercice}

\begin{exercice}
\label{grammanbncn}
Montrer que la grammaire suivante engendre le langage $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$

\[
\begin{cases}
S \rightarrow XY \\
X \rightarrow aXbZ~|~\epsilon\\
Zb \rightarrow bZ \\
ZY \rightarrow Yc \\
Y \rightarrow \epsilon
\end{cases}
\]
\end{exercice}

\section{Hierarchie de Chomsky}

Chomsky propose en 1956 une classification des grammaires en fonction des formes de leurs règles. Les grammaires sont donc divisées en 4 catégories, appelées grammaires de type 0, 1, 2 et 3.

\subsection{Grammaires de type 3, ou régulières}

Les grammaires de type 3, appelées \textbf{grammaires régulières}, se divisent elles-même en deux sous-classes : les \textbf{grammaires linéaires à gauche} et \textbf{grammaires linéaires à droite}.

\subsubsection{Grammaires linéaires à gauche et à droite}

Dans les deux cas, on ne permet que d'écrire un symbole terminal à la fois, en allant toujours dans le même sens.

\begin{definition}{Grammaire linéaire à gauche}{}
Une grammaire linéaire à gauche est une grammaire dont toutes les règles sont de la forme 
\begin{itemize}
\item[] $A \rightarrow Ba$
\item[] ou
\item[] $A \rightarrow a$
\item[] ou
\item[] $A \rightarrow \epsilon$
\end{itemize}

où $A$ et $B$ sont des symboles non-terminaux, et $a$ un terminal.
\end{definition}

\begin{example}
\label{gramling}
La grammaire 
\[
\begin{cases}
S \rightarrow Sa~|~Ta~|~a \\
T \rightarrow Tb~|~b
\end{cases}
\]
est une grammaire linéaire à gauche. 
\end{example}

\begin{exercice}
Quel est le langage engendré par la grammaire de l'exemple \ref{gramling} ?
\end{exercice}


\begin{definition}{Grammaire linéaire à droite}{}
Une grammaire linéaire à droite est une grammaire dont toutes les règles sont de la forme 
\begin{itemize}
\item[] $A \rightarrow aB$
\item[] ou
\item[] $A \rightarrow a$
\item[] ou
\item[] $A \rightarrow \epsilon$
\end{itemize}

où $A$ et $B$ sont des symboles non-terminaux, et $a$ un terminal.
\end{definition}

\begin{example}
\label{gramlind}
La grammaire 
\[
\begin{cases}
S \rightarrow aS~|~bS~|~\epsilon
\end{cases}
\]
est une grammaire linéaire à droite. 
\end{example}


\begin{exercice}
Quel est le langage engendré par la grammaire de l'exemple \ref{gramlind} ?
\end{exercice}

\paragraph{Remarque} Les grammaires de type 3, ou régulières, est l'ensemble des grammaires linéaires à gauche et des grammaires linéaires à droite, mais sans mélange. Par exemple, la grammaire 
\[
\begin{cases}
S \rightarrow aS~|~Sb~|~\epsilon
\end{cases}
\]
\textbf{n}'est \textbf{pas} une grammaire de type 3. 

\begin{lemma}
Les grammaires linéaires à gauche et les grammaires linéaire à droite ont la même expressivité. Dit autrement, tout langage engendré par une grammaire linéaire à gauche est est également engendré par une grammaire linéaire à droite, et inversement.
\end{lemma}

\begin{proof}
On n'entrera pas ici dans les détails, qui dépassent le cadre de ce cours. Etant donnée une grammaire linéaire à gauche (resp. droite), on peut écrire une grammaire linéaire à droite (resp. gauche) qui la simule en partant des règles $A \rightarrow a$ et $A \rightarrow \epsilon$. Les règles \textit{normales} sont inversées et nouvelles règles \textit{terminales} correspondent à l'axiome de la première grammaire. 
\end{proof}

\begin{exercice}(**) Donner, en suivant l'intuition de la preuve ci-dessus, une grammaire linéaire à droite qui engendre le même langage que la grammaire de l'exemple \ref{gramling}.
\end{exercice}

\subsubsection{Une extension du théorème de Kleene}
On a vu dans le chapitre \ref{hierarchie} que les langages pouvant être décrits par des expressions rationnelles et ceux définissables par automate étaient les mêmes. Ce résultat peut s'étendre aux grammaires de type 3.

\begin{theorem}
\label{kleeneplusplus}
Les langages reconnus par automate fini sont exactement ceux engendrés par des grammaires de type 3.
\end{theorem}

\begin{proof}
Comme dans le théorème de Kleene original, on propose comme preuve une tradition des grammaires vers les automates, et inversement. Contrairement aux expressions rationnelles, les grammaires de type 3 ont un fonctionnement très similaire aux automates finis, les traductions sont donc très simples.

Soit $G$ une grammaire de type 3. Puisque les grammaires linéaires à gauche et à droite engendrent les mêmes langages, on peut supposer que $G$ est une grammaire linéaire à droite. On crée un automate dont les états sont les symboles non-terminaux de $G$, ainsi qu'un état supplémentaire et terminal $T$ (pour terminus). L'état initial est son axiome. Toute règle $A \rightarrow aB$ devient une transition de $A$ vers $B$ en lisant $a$. Les règles $A \rightarrow a$ deviennent des transitions de $A$ vers $T$ en $a$. Enfin, on rend terminal tout état $A$ tel qu'il existe une règle $A \rightarrow \epsilon$.

Soit $A$ un automate fini. On déterminise $A$ en $A_{det}$. On crée une grammaire dont les symboles non-terminaux sont les états de $A_{det}$. L'axiome est l'état initial de $A_{det}$. Toute transition $A \xrightarrow{a} B$ se traduit par une règle $A \rightarrow aB $. Pour tout état terminal $A$, on ajoute une règle $A \rightarrow \epsilon$.
\end{proof}

\begin{example}
Soit la grammaire suivante :
\[
\begin{cases}
S \rightarrow aS~|~aA~|~bA~|~a \\
A \rightarrow aB~|~b~|~\epsilon \\
B \rightarrow b~|~aS~|~bS 
\end{cases}
\]

Le langage qu'elle engendre est reconnu par l'automate suivant :


\begin{figure}[!ht]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.9cm,
                    semithick]
  \tikzstyle{every state}=[fill=white,text=black]
  \tikzstyle{place}=[rectangle,draw=black,fill=white, minimum size=7mm]


  \node[initial,state] (S)                    {$S$};
  \node[state,accepting] (A)      [right of=S]                {$A$};  
  \node[state] (B)      [right of=A]                {$B$};   
  \node[state,accepting] (T)      [above of=A]                {$T$};    

  \path %(I) edge[loop above]              node {$a,b$} (I)
(S) edge      [loop above]        node {$a$} (S)
(S) edge      []        node {$a,b$} (A)
(S) edge      []        node {$a$} (T)
(A) edge      []        node {$b$} (T)
(A) edge      []        node {$a$} (B)
(B) edge      []        node {$b$} (T)
(B) edge      [bend left]        node {$a,b$} (S);
\end{tikzpicture}
\end{figure}

\end{example}

\begin{example}
L'automate de l'exemple \ref{autopasaba} est simulé par la grammaire qui suit, où $0$ est remplacé par $A$, $1$ par $B$ etc, et $A$ est l'axiome.

\[
\begin{cases}
A \rightarrow bA~|~aB~|~ \epsilon \\
B \rightarrow aB~|~bC~|~ \epsilon \\
C \rightarrow aD~|~bA~|~ \epsilon \\
D \rightarrow aD~|~bD
\end{cases}
\]

\end{example}

\begin{exercice}
Traduire en automate la grammaire 
\[
\begin{cases}
S \rightarrow aS~|~bB~|~a~|~\epsilon \\
A \rightarrow aS~|~\epsilon \\
B \rightarrow a
\end{cases}
\]

\end{exercice}

\begin{exercice}
Traduire en grammaire l'automate de l'exemple \ref{detex1}.
\end{exercice}



\subsection{Grammaires de type 2, ou non contextuelles}

Les grammaires de type 2, appelées \textbf{grammaires non contextuelles} (\textit{context-free}), ou parfois grammaires algébriques, n'ont aucune contrainte quant à la partie droite des règles. Elles imposent cependant que la partie gauche ne contienne pas plus d'un caractère, empêchant donc de prendre en compte le contexte dans les dérivations.

\begin{definition}{Grammaire non contextuelle}{}
Une grammaire non contextuelle est une grammaire dont toutes les règles sont de la forme 
\begin{itemize}
\item[] $A \rightarrow \gamma$
\end{itemize}
où $A$ est un symbole non-terminal, et $\gamma$ est n'importe quel mot constitué de symboles terminaux et non-terminaux.
\end{definition}

\begin{example}
\label{gramanbn}
La grammaire 

\[
\begin{cases}
S \rightarrow aSb \\
S \rightarrow \epsilon
\end{cases}
\]

est une grammaire de type 2, qui engendre le langage $\{a^nb^n ~|~n \in \mathbb{N}\}$
\end{example}

\paragraph{Remarque} Toute grammaire de type 3 est également une grammaire de type 2, puisque toute règle d'une grammaire de type 3 a bien un seul symbole à gauche de toute règle.

\begin{lemma}
Les langages engendrés par des grammaires de type 3 forment un sous-ensemble strict des langages engendrés par des grammaires de type 2. Autrement dit, si un langage est engendré par une grammaire de type 3, alors il l'est également par une grammaire de type 2, mais il existe au moins un (et même plein) langage engendré par une grammaire de type 2 qui ne peut pas être engendré par une grammaire de type 3.  
\end{lemma}

\begin{proof}
L'inclusion non-stricte est triviale avec la remarque précédente. En effet, soit $L$ un langage engendré par la grammaire de type 3 $G$, alors $G$ est également une gramaire de type 2 qui, de fait, engendre toujours $L$.

Pour l'aspect strict de cette inclusion, la grammaire de type 2 de l'exemple \ref{gramanbn} engendre le langage $\{a^nb^n ~|~ n \in \mathbb{N}\}$. Or, on a vu en \ref{limiterecauto} que ce langage n'était pas reconnaissable par automate. Puisque les grammaires de type 3 ont la même expressivité que ces derniers (théorème \ref{kleeneplusplus}), il n'en existe pas qui engendre ce langage.
\end{proof}




\subsection{Grammaires de type 1, ou contextuelles}

Les grammaires de type 1, appelées \textbf{grammaires contextuelles} (\textit{context-sensitive}) permettent de mettre n'importe quel mot (contenant au moins un symbole non-terminal) à gauche d'une flèche. La seule contrainte est désormais qu'on ne permet pas d'avoir le mot vide à droite d'une flèche, sauf cas particulier pour capturer le mot vide.

\begin{definition}{Grammaire contextuelle}{}
Une grammaire contextuelle est une grammaire dont toutes les règles sont de la forme 
\begin{itemize}
\item[] $\alpha \rightarrow \beta$, où $\alpha$ et $\beta$ sont des mots constitué de symboles terminaux et non-terminaux, tels que $\alpha$ contient au moins un symbole non-terminal et $\beta \neq \epsilon$
\item[] ou
\item[] $S \rightarrow \epsilon$, où $S$ est l'axiome de la grammaire. Si cette règle est utilisée, $S$ n'a pas le droit d'apparaître à droite d'une règle.
\end{itemize}

\end{definition}

Cette exception un peu étrange permet aux grammaires contextuelles d'engendrer le mot vide, ce qui ne serait pas possible uniquement avec le premier type de règle. Supposons par exemple qu'on ait une grammaire contextuelle $G$ d'axiome $S$ engendrant un langage $L$ ne contenant pas le mot vide, on peut facilement construire la grammaire $G'$ qui engendre $L \cup \{\epsilon\}$. 

Il suffit d'ajouter un nouveau non-terminal, $S'$, qui sera le nouvel axiome, ainsi que les deux règles qui suivent : 


\[
\begin{cases}
S' \rightarrow \epsilon \\
S' \rightarrow S
\end{cases}
\]

Ainsi, on permet d'engendrer $\epsilon$ dès le début, ou de faire une génération "normale" de $G$ en repartant de $S$. Avec cette transformation, on est sûr que $S'$ n'apparaît jamais à droite d'une règle.

\begin{example}
\label{grammanbncnbis}
La grammaire de l'exercice \ref{grammanbncn} \textbf{n}'est \textbf{pas} une grammaire de type 1, à cause des règles $X \rightarrow \epsilon$ et $Y \rightarrow \epsilon$. On peut cependant légèrement la modifier pour obtenir une grammaire de type 1 qui engendre le même langage, c'est-à-dire $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$


\[
\begin{cases}
S \rightarrow XY~|~\epsilon \\
X \rightarrow aXbZ~|~abZ\\
Zb \rightarrow bZ \\
ZY \rightarrow Yc \\
bY \rightarrow b
\end{cases}
\]

\end{example}

\begin{exercice}
Montrer que la grammaire de l'exemple \ref{grammanbncnbis} engendre bien le langage \newline $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$.
\end{exercice}

\paragraph{Remarque} Une grammaire de type 2 \textbf{n}'est \textbf{pas forcément} une grammaire de type 1, à cause des règles de la forme $A \rightarrow \epsilon$. Il existe cependant un algorithme qui prend une grammaire de type 2 et en renvoie une autre, également de type 2, engendrant le même langage sans règle de la forme $A \rightarrow \epsilon$, sauf éventuellement pour l'axiome. Cet algorithme sera décrit en ???. %TODO


\begin{lemma}
Les langages engendrés par des grammaires de type 2 forment un sous-ensemble strict des langages engendrés par des grammaires de type 1. Autrement dit, si un langage est engendré par une grammaire de type 2, alors il l'est également par une grammaire de type 1, mais il existe au moins un (et même plein) langage engendré par une grammaire de type 1 qui ne peut pas être engendré par une grammaire de type 2.  
\end{lemma}

\begin{proof}
L'inclusion non-stricte est encore une fois simple avec la remarque précédente. En effet, soit $L$ un langage engendré par la grammaire de type 2 $G$, alors on peut construire $G'$ une gramaire de type 2 qui est également de type 1 et engendre toujours $L$.

Pour l'aspect strict de cette inclusion, la grammaire de type 1 de l'exemple \ref{grammanbncnbis} engendre le langage $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$. On n'en donnera malheureusement pas la preuve ici, mais ce langage n'est pas engendrable par une grammaire de type 2\footnote{Vous pouvez cependant essayer de vous en convaincre en essayant d'écrire une telle grammaire !}.
\end{proof}






\subsection{Grammaires de type 0, ou générales}

Les grammaires de type 0, appelées \textbf{grammaires générales}, sont tout simplement l'ensemble des grammaires formelles. Elles n'ont donc aucune contrainte, sinon toujours celle d'avoir au moins un symbole non-terminal à gauche de toute règle.
Il va sans dire que toute grammaire présenté jusqu'ici ou dans la suite de ce document constitue une grammaire de type $0$. 

\begin{lemma}
Les langages engendrés par des grammaires de type 1 forment un sous-ensemble strict des langages engendrés par des grammaires de type 0. Autrement dit, si un langage est engendré par une grammaire de type 1, alors il l'est également par une grammaire de type 0, mais il existe au moins un (et même plein) langage engendré par une grammaire de type 0 qui ne peut pas être engendré par une grammaire de type 1.  
\end{lemma}

\begin{proof}
L'inclusion non-stricte est triviale, puisque toute grammaire de type 1 est également une grammaire de type 0.

Pour l'aspect strict de cette inclusion, c'est un peu plus compliqué, puisqu'il n'existe pas - à notre connaissance - d'exemple très concret et naturel de langage strictement engendrable par une grammaire de type 0. On renverra par contre à \href{https://cs.stackexchange.com/a/56634}{cet exemple abstrait et très amusant}\footnote{https://cs.stackexchange.com/a/56634}. Informellement, l'idée est de faire une liste (infinie) des mots sur un alphabet, ainsi qu'une liste (également infinie) des grammaires de type 1 sur le même alphabet (c'est un peu technique, mais on peut écrire un algorithme qui parcourt cet ensemble sans rien oublier). 

On aligne ces deux listes, cad. qu'on a maintenant une liste de paires (un mot et une grammaires, associés arbitrairement). On définit maintenant $L$ le langage des mots qui n'appartiennent pas au langage engendré par la grammaire qui leur est associée. Sans entrer dans les détails, ce langage est bien engendré par une grammaire de type 0 consistant en gros en un "simulateur de grammaire de type 1".

Il n'est par contre pas engendré par une grammaire de type $1$. En effet, si une telle grammaire $G$ existait, elle serait à une position $j$ de la liste des grammaires. Dans ce cas, le $j^{eme}$ mot appartient à $L$ ssi. il n'appartient pas à $L_G$. La grammaire $G$ ne peut donc en fait pas engendrer $L$.
\end{proof}

\section{Utilisation de grammaires algébriques}

On finit ce chapitre en présentant quelques spécificités des grammaires algébriques, ou de type 2.

\subsection{Arbres de dérivation}

Puisque les grammaires algébriques ne peuvent pas avoir plus d'un symbole à gauche de leurs règles, leurs dérivations se représentent efficacement sous forme d'arbre, où chaque noeud interne est un symbole non-terminal, dont les descendants sont les symboles produits par application d'une règle. Le mot engendré est alors la concaténation, de la gauche vers la droite (parcours préfixe) des symboles aux feuilles.

\begin{example}
\label{gramparen}
Soit la grammaire suivante, où "(" et ")" sont des symboles terminaux :
\[
\begin{cases}
S \rightarrow (S)S ~|~ \epsilon
\end{cases}
\]

Le mot \textcolor{blue}{(}\textcolor{red}{()}\textcolor{blue}{)}\textcolor{orange}{()} peut par exemple être dérivé de la façon suivante :

\begin{figure}[H]
\center
\Tree[.S {\textcolor{blue}{(}} [.S {\textcolor{red}{(}} [.S {$\epsilon$} ] {\textcolor{red}{)}} [.S {$\epsilon$}  ]  ] {\textcolor{blue}{)}} [.S {\textcolor{orange}{(}} [.S {$\epsilon$}  ] {\textcolor{orange}{)}} [.S {$\epsilon$}  ]  ]  ]
\end{figure}

\end{example}

\begin{exercice}
Quel est le langage engendré par la grammaire de l'exemple \ref{gramparen} ?
\end{exercice}

\begin{exercice}
Donner un arbre de dérivation du mot abbbaaba dans la grammaire de l'exemple \ref{grammLR}.
\end{exercice}

Cette représentation ne prend pas en compte l'ordre dans lequel les règles sont appliquées. Mais vu que les règles des grammaires algébriques ne peuvent pas prendre en compte le contexte, l'ordre importe peu, on reviendra toujours au même, par exemple 

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,
                    semithick]
  \tikzstyle{every state}=[fill=white,text=black]
  \tikzstyle{place}=[rectangle,draw=black,fill=white, minimum size=7mm]


  \node[] (0)                    {$(S)S$};
  \node[] (1)      [below left of=0]                {$()S$};  
  \node[] (2)      [below right of=0]                {$(S)$};
  \node[] (3)      [below=3cm of 0]                {$()$};  
 
  \path %(I) edge[loop above]              node {$a,b$} (I)
(0) edge      []        node{} (1)
(0) edge      []        node{} (2)
(1) edge      []        node{} (3)
(2) edge      []        node{} (3)
;

\end{tikzpicture}
\end{figure}

En plus d'être très lisible, les arbres de dérivations donnent une décompostion structurelle du mot dérivé. La structure d'un mot (ou d'une phrase) est évidemment cruciale pour l'interprétation. Soit par exemple la grammaire suivante, où "$+$", "$\times$", "(" et ")" sont des symboles terminaux et $n$ représente n'importe quel nombre.

\[
\begin{cases}
S \rightarrow S + S ~|~ S \times S ~|~ (S) ~|~ n
\end{cases}
\]

Sans définir de priorité sur les opérateurs, le mot $n + n \times n$ est ambigü. Mais, si on en regarde la dérivation (cf. la figure \ref{deuxderivs}), on comprend s'il s'agit de $n + (n \times n)$ (arbre gauche) ou $(n + n) \times n$ (arbre droit).


\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \Tree[.S [.S n ] {$+$} [.S [.S n ] {$\times$} [.S n ] ] ]
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \Tree[.S [.S [.S n ] {$+$} [.S n ] ] {$\times$} [.S n ] ]
    \end{minipage}
	\caption{Deux dérivations de $n + n \times n$}
	\label{deuxderivs}
\end{figure}

Pour des exemples plus linguistiques, on renvoie évidemment le lecteur ou la lecture à ses cours de syntaxe.

\begin{definition}{Grammaire algébrique ambiguë}{}
Une grammaire algébrique est dite \textbf{ambiguë} ssi. son langage engendré contient au moins un mot admettant deux arbres de dérivation différents. 
\end{definition}

\begin{definition}{Pouvoir génératif}{}
Deux grammaires algébriques ont le même \textbf{pouvoir génératif faible} ssi elles engendrent les mêmes mots, mais pas forcément avec le même langage. Elles ont le même \textbf{pouvoir génératif fort} ssi. elles engendrent des arbres équivalents.
\end{definition}

\begin{example}
La grammaire 


\[
\begin{cases}
S \rightarrow S + T ~|~ S \times T ~|~ T \\
T \rightarrow (S) ~|~ n
\end{cases}
\]

n'est pas ambiguë : le mot $n + n \times n$ a une seule dérivation, où l'addition est effectuée avant la multiplication\footnote{Cette grammaire n'est donc pas une bonne représentation de l'arithmétique standard, puisqu'elle n'en respecte pas les priorités. Une bonne grammaire algébrique faisant ça existe, mais est plus compliquée. En pratique, si vous écrivez avec un outil comme yacc un parser sous la forme d'une grammaire algébrique, vous pourrez écrire une telle grammaire de façon naïve en précisant les priorités entre opérateurs, et une bonne grammaire sera générée automatiquement.}. Si on veut que la multiplication soit effectuée avant l'addition, on est obligé de faire apparaître des parenthèses :


\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \Tree[.S [.S [.T n ] + [.T n ] ] {$\times$} [.T n ] ]
        \caption{Dérivation de $n + n \times n$}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
         \Tree[.S [.S [.T n ] ] + [.T ( [.S [.S [.T n ] ] {$\times$} [.T n ] ] ) ] ]
		 \caption{Dérivation de $n + (n \times n)$}
    \end{minipage}
\end{figure}

Les deux grammaires d'opérations arithmétiques présentées ont le même pouvoir génératif faible, mais pas fort.

\end{example}

\begin{definition}{Langage intrinsèquement ambigü}{}
Un langage algébrique (ie. engendré par une grammaire algébrique) est \textbf{intrinsèquement ambiguë} ssi. toutes les grammaires algébriques le reconnaissant sont ambigüs.
\end{definition}

\begin{example}
Le langage des opérations arithmétiques décrit plus haut est engendré par au moins une grammaire ambiguë, mais aussi une non-ambiguë. Ce langage n'est donc pas intrinsèquement ambiguë. 
\end{example}

\begin{example}
Le langage $\{a^nba^mba^pba^q ~|~ n,m,p,q \in \mathbb{N} \text{ et } ((n \geq m \text{ et } p \geq q) \text{ ou } (n \geq q \text{ et } m \geq p))\}$. Voir le DM2 pour la justification.
\end{example}

\subsection{Transformation de grammaires algébriques}

On présente ici quelques algorithmes pour nettoyer des grammaires algébriques.

\begin{definition}{Grammaire algébrique propre}{}
Une grammaire algébrique est dite \textbf{propre} ssi. elle est \textbf{$\epsilon$-libre}, \textbf{sans cycle} et \textbf{dépourvue de symboles inutiles}.
\end{definition}

\begin{definition}{Grammaire $\epsilon$-libre}{}
Une grammaire est \textbf{$\epsilon$-libre} ssi. elle ne possède pas de règle de la forme $\alpha \rightarrow \epsilon$, sauf éventuellement $S \rightarrow \epsilon$, auquel cas l'axiome $S$ ne doit jamais apparaître à droite d'une règle.
\end{definition}

\begin{definition}{Grammaire sans cycle}{}
Une grammaire est \textbf{$\epsilon$-libre} ssi. elle ne permet pas de dérivation de la forme $A \rightarrow^+ A$, où $A$ est un symbole non-terminal. 
\end{definition}

\begin{example}
La grammaire 

\[
\begin{cases}
S \rightarrow AB~|~C\\
A \rightarrow C \\
B \rightarrow \epsilon \\
C \rightarrow S~|~a 
\end{cases}
\]

possède des cycles en $S$ et $A$ :

\[
S \rightarrow AB \rightarrow A \rightarrow C \rightarrow S
\]

\[
A \rightarrow C \rightarrow S \rightarrow AB \rightarrow A
\]
\end{example}

\begin{definition}{Symbole inutile}{}
Un symbole inutile est un symbole non-terminal qui est soit \textbf{inaccessible}, soit \textbf{sans-production}. 

Un symbole $A$ est inaccessible ssi. il n'existe pas de dérivation $S \rightarrow^* \alpha A \beta$, cad. qu'on ne peut pas produire de $A$ en partant de l'axiome. 

Un symbole $A$ est sans production ssi. il n'existe pas de dérivation $A \rightarrow^* w$ où $w \in \Sigma^*$, cad. qu'on ne peut pas dériver un "vrai mot" à partir de $A$.
\end{definition}

\begin{example}
Dans la grammaire 

\[
\begin{cases}
S \rightarrow A~|~B \\
A \rightarrow CE \\
B \rightarrow \epsilon \\
C \rightarrow a \\
D \rightarrow Fa~|~\epsilon \\
E \rightarrow abbA \\
F \rightarrow bD
\end{cases}
\]

possède deux symboles inutiles. En effet, $A$ est sans production (après en avoir produit un, on doit "résoudre" un $D$, qui lui-même produit un $A$ et ainsi de suite), et $D$ et $F$ sont inaccessibles.

\end{example}

\subsubsection{Elimination d'$\epsilon$-productions}

On doit d'abord calculer l'ensemble $K$ des symboles non-terminaux $A$ tels que $A \rightarrow^* \epsilon$, cad. les symboles qui peuvent se réécrire en $\epsilon$.

Déjà, pour toute règle de la forme $A \rightarrow \epsilon$, on met évidemment $A$ dans $K$. On procède ensuite par point fixe, en inspectant les différentes règles. A chaque règle $B \rightarrow \alpha$, si $\alpha$ est composé uniquement de symboles de $K$, alors $\alpha$ peut se réécrire en $\epsilon$, on ajoute donc $B$ à $K$. On refait une passe sur l'ensemble des règles tant qu'on a des ajouts à $K$.

\begin{example}
\label{gramelimeps1}
Soit la grammaire 

\[
\begin{cases}
S \rightarrow CD~|~AB~|~E \\
A \rightarrow a~|~BC \\
B \rightarrow abba~|~\epsilon \\
C \rightarrow BB \\
D \rightarrow Sa \\
E \rightarrow AaBEB~|~BSb
\end{cases}
\]

Le symbole $B$ peut se réécrire directement en $\epsilon$, on l'ajoute donc à $K$. On fait maintenant une première passe sur les différentes règles et on remarque que $C$ peut se réécrire en $BB$. Puisque $B \in K$, $B$ peut se réécrire en $\epsilon$, ce qui implique que $BB$ peut se réécrire en $\epsilon \epsilon = \epsilon$. On ajoute donc $C$ à $K$.

Puisqu'on a eu un ajout à $K$, on refait une passe sur les règles. $A$ peut se réécrire en $BC$, donc en $\epsilon$. On ajoute $A$ à $K$.

Nouvelle passe, on ajoute $S$ à $K$ grâce à la règle $S \rightarrow AB$. 

Enfin, notre dernière passe ne révèle pas de nouveau symbole à ajouter à $K$. On a donc atteint notre point fixe $K = \{S,A,B,C\}$.
\end{example}

Puisqu'on veut effacer les règles de la forme $A \rightarrow \epsilon$ tout en engendrant le même langage, on doit modifier les règles pour rattraper les mots éventuellement perdus. L'astuce va être de dupliquer toute règle  en "remplaçant potentiellement" chaque symbole de $K$ qu'elle contient par $\epsilon$.

\begin{example}
On reprend la grammaire et l'ensemble $K$ de l'exemple \ref{gramelimeps1}. 

On commence par la règle $S \rightarrow CD$. Puisque $C$ appartient à $K$, il peut se réécrire en $\epsilon$, on intègre donc cette possibilité en ajoutant une règle $S \rightarrow D$. De même, la règle $S \rightarrow AB$ va être dupliquée en $S \rightarrow A$ et $S \rightarrow B$. Les symboles $A$ et $B$ peuvent se réécrire en $\epsilon$ lors d'une même dérivation, mais puisque le but de l'algorithme est d'éliminer les règles de la forme $A \rightarrow \epsilon$, on ne va évidemment pas ajouter de règle $S \rightarrow \epsilon$.

Pour les mêmes raisons, on va ajouter les règles $A \rightarrow B$, $A \rightarrow C$, $C \rightarrow B$, $D \rightarrow a$. 

La règle $E \rightarrow AaBEC$ contenant 3 occurrences de symboles de $K$ sur sa droite, on va devoir avoir $2^3 = 8$ copies de la règle, pour prendre en compte toutes les combinaisons de "deviendra $\epsilon$, deviendra pas $\epsilon$". On a donc les règles 

\begin{itemize}
\item[] $E \rightarrow AaBEB$ (déjà présent)
\item[] $E \rightarrow \textcolor{white}{A}aBEB$
\item[] $E \rightarrow Aa\textcolor{white}{B}EB$
\item[] $E \rightarrow AaBE\textcolor{white}{B}$
\item[] $E \rightarrow \textcolor{white}{A}a\textcolor{white}{B}EB$
\item[] $E \rightarrow \textcolor{white}{A}aBE\textcolor{white}{B}$
\item[] $E \rightarrow Aa\textcolor{white}{B}E\textcolor{white}{B}$
\item[] $E \rightarrow \textcolor{white}{A}a\textcolor{white}{B}E\textcolor{white}{B}$
\end{itemize}

De même, on ajoute les règles $E \rightarrow Bb~|~Sb~|~b$.

Notre grammaire est donc maintenant 
\[
\begin{cases}
S \rightarrow CD~|~AB~|~E\textcolor{blue}{~|~D~|~A~|~B} \\
A \rightarrow a~|~BC\textcolor{blue}{~|~B~|~C} \\
B \rightarrow abba~\text{\st{$~|~\epsilon$}}\\
C \rightarrow BB\textcolor{blue}{~|~B} \\
D \rightarrow Sa\textcolor{blue}{~|~ a} \\
E \rightarrow AaBEB~|~BSb~|~\textcolor{blue}{aBEB~|~AaEB~|~AaBE~|~aEB~|~aBE~|~AaE~|~aE~|~Sb~|~Bb~|~b}
\end{cases}
\]

\end{example}

Il reste cependant une étape. En effet, La grammaire originelle engendrait notamment le mot vide, ce qui n'est plus le cas de la nouvelle. La troisième et dernière étape est est, si l'axiome $S$ de la grammaire étudiée appartient à $K$, d'engendrer $\epsilon$ comme cas particulier. Pour ça, on ajoute un nouveau symbole $S'$ qui remplace $S$ comme axiome, ainsi que les règles $S' \rightarrow S~|~\epsilon$.

\begin{example}

Au final, la version sans $\epsilon$-production de la grammaire de l'exemple \ref{gramelimeps1} est donc 

\[
\begin{cases}
S' \rightarrow S~|~\epsilon \\
S \rightarrow CD~|~AB~|~E\textcolor{black}{~|~D~|~A~|~B} \\
A \rightarrow a~|~BC\textcolor{black}{~|~B~|~C} \\
B \rightarrow abba \\
C \rightarrow BB\textcolor{black}{~|~B} \\
D \rightarrow Sa\textcolor{black}{~|~ a} \\
E \rightarrow AaBEB~|~BSb~|~\textcolor{black}{aBEB~|~AaEB~|~AaBE~|~aEB~|~aBE~|~AaE~|~aE~|~Sb~|~Bb~|~b}
\end{cases}
\]

où S' est l'axiome.

\end{example}

\begin{exercice}
Eliminer les $\epsilon$-production de la grammaire suivante :

\[
\begin{cases}
S \rightarrow ABC~|~BD \\
A \rightarrow BC~|~ab~|~ \\
B \rightarrow CDC~|~\epsilon \\
C \rightarrow BAB~|~ABA \\
D \rightarrow abba~|~BB~|~BCBS
\end{cases}
\]
\end{exercice}


\subsubsection{Elimination de cycles}

Si la grammaire est $\epsilon$-libre, le seul moyen d'avoir des cyles est via des règles de la forme $A \rightarrow B$, ou \textbf{productions singulières}. Puisqu'on a un algorithme pour éliminer les $\epsilon$-productions, on peut supposer que la grammaire étudiée est $\epsilon$-libre. L'élimination de cyles peut donc être ramenée à l'élimination des productions singulières.

La première étape de l'algorithme consiste à trouver l'ensemble des paires de non-terminaux $(A,B)$ telles que $A \rightarrow^* B$, cad l'ensemble des symboles qui peuvent se réécrire en un autre non-terminal. On procède encore une fois par point fixe sur les règles suivantes : 

\begin{itemize}
\item Si $A \rightarrow B$, alors $A \rightarrow^+ B$
\item Si $A \rightarrow^+ B$ et $B \rightarrow C$, alors $A \rightarrow^+ C$
\end{itemize}

\begin{example}
\label{gramelimcycles}
Soit la grammaire 

\[
\begin{cases}
S \rightarrow B~|~CA \\
A \rightarrow C~|~a \\
B \rightarrow S~|~C~|~ab \\
C \rightarrow c 
\end{cases}
\]

On obtient ces relations :

\begin{itemize}
\item $S \rightarrow^+ B$ (règle 1)
\item $S \rightarrow^+ C$ (règle 1 puis règle 2)
\item $A \rightarrow^+ C$ (règle 1)
\item $B \rightarrow^+ S$ (règle 1)
\item $B \rightarrow^+ C$ (règle 1)
*\end{itemize}

\end{example}

Une fois qu'on a identifié ces paires $(A,B)$, il suffit d'\textit{inliner}, cad. de copier/coller les productions non-singulières de $B$ dans celles de $A$. Ce faisant, on permet de faire immédiatement des réécritures qui se faisaient sinon en plusieurs étapes. 

\begin{example}
En continuant l'exemple \ref{gramelimcycles}, on obtient 

\[
\begin{cases}
S \rightarrow \text{\st{$B~|~$}}~CA \textcolor{blue}{~|~ab~|~c} \\
A \rightarrow \text{\st{$C~|~$}}~a \textcolor{blue}{~|~c}\\
B \rightarrow \text{\st{$S~|~C~|~$}}~ab \textcolor{blue}{~|~c} \\
C \rightarrow c
\end{cases}
\]

\end{example}