\chapter{Grammaires formelles et hiérarchie de Chomsky}
\label{grammaires}


\section{Principe général}



\begin{exercice}
Quel est le langage engendré par la grammaire qui suit ? 

\[
\begin{cases}
S \rightarrow LaaR \\
L \rightarrow Lb \\
L \rightarrow ab \\
R \rightarrow bR \\
R \rightarrow ba
\end{cases}
\]

\end{exercice}

\begin{correction*}
Le symbole $L$ peut générer autant de $b$ qu'il veut sur sa droite, avant se s'arrêter en produisant un $ab$. Le symbole $L$ engendre donc le langage $abb^* = ab^+$. De même, $R$ engendre $b^+a$. Puisque l'axiome peut uniquement produire $LaaR$, le langage engendré par la grammaire est $ab^+aab^+a$.
\end{correction*}

\begin{exercice}
\label{grammab}
Quel est le langage engendré par la grammaire suivante ?

\[
\begin{cases}
S \rightarrow A~|~B \\
A \rightarrow aA~|~\epsilon \\
B \rightarrow bB~|~\epsilon
\end{cases}
\]

\end{exercice}

\begin{correction*}
Le symbole $A$ produit autant de $a$ qu'il veut avant de disparaître. Le langage engendré par $A$ est donc $a^*$, de même que $B$ engendre $b^*$. Puisqu'on a le choix entre réécrire l'axiome $S$ en $A$ ou $B$, on engendre $a^*+b^*$.
\end{correction*}

\begin{exercice}
\label{grammsigma}
Quel est le langage engendré par la grammaire suivante ?

\[
\begin{cases}
S \rightarrow aS~|~bS~|~\epsilon 
\end{cases}
\]

\end{exercice}

\begin{correction*}
Cette fois, on peut engendrer un mélange de $a$ et de $b$, autant qu'on veut, dans l'ordre que l'on veut (règles 1 et 2), puis arrêter (règle 3). Le langage engendré est donc celui de tous les mots, cad. $(a+b)^*$.
\end{correction*}

\begin{exercice}
Donner l'ensemble des mots qui admettent deux dérivations (ie. peuvent être construits de plusieurs façons différentes) dans la grammaire de l'exercice \ref{grammab}. Même question pour celle de l'exercice \ref{grammsigma}.
\end{exercice}

\begin{correction*}
Dans la première grammaire, on peut engendrer $\epsilon$ de deux façons différentes :

\begin{itemize}
\item $S \rightarrow A \rightarrow \epsilon$
\item $S \rightarrow B \rightarrow \epsilon$
\end{itemize}

Aucun n'autre mot ne dispose de plusieurs dérivations. Toute dérivation est de la forme 

\begin{itemize}
\item $S \rightarrow A \rightarrow^n a^nA \rightarrow a^n$
\item[] ou
\item $S \rightarrow B \rightarrow^n b^nB \rightarrow b^n$
\end{itemize}

Deux dérivations différentes qui commencent toutes les deux par $S \rightarrow A$ doivent utiliser un nombre différent de fois la règle produisant un $a$, ce qui implique de générer des mots de longueurs différentes et donc des mots qui le sont également. Idem pour deux dérivations qui commencent par $S \rightarrow B$.

Deux dérivations différentes qui commencent par $S \rightarrow A$ et $S \rightarrow B$ ne peuvent pas générer le même mot à part $\epsilon$, car la présence de $a$ exclut celle de $b$, et inversement. 

Dans la deuxième grammaire, il n'existe qu'une dérivation par mot. En effet, soit $w = c_1c_2...c_n$, la seule possibilité est de générer $c_n$, puis $c_{n-1}$, ... et enfin $c_1$, cad. de produire linéairement le mot de la droite vers la gauche.
\end{correction*}

 
 \section{Formalisation}
 

\begin{exercice}
Donner une grammaire qui engendre le langage $\{a^nb^n ~|~ n \in \mathbb{N}\}$
\end{exercice}

\begin{correction*}

Ce langage est engendré par 
\[
\begin{cases}
S \rightarrow aSb ~|~ \epsilon 
\end{cases}
\]

Exemple d'application :

\[
S \rightarrow \textcolor{blue}{a}S\textcolor{blue}{b} \rightarrow \textcolor{blue}{a}\textcolor{red}{a}S\textcolor{red}{b}\textcolor{blue}{b} \rightarrow \textcolor{blue}{a}\textcolor{red}{a}\textcolor{orange}{a}S\textcolor{orange}{b}\textcolor{red}{b}\textcolor{blue}{b} \rightarrow \textcolor{blue}{a}\textcolor{red}{a}\textcolor{orange}{a}\textcolor{orange}{b}\textcolor{red}{b}\textcolor{blue}{b} 
\]

\end{correction*}

\begin{exercice}
\label{grammanbncn}
Montrer que la grammaire suivante engendre le langage $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$

\[
\begin{cases}
S \rightarrow XY \\
X \rightarrow aXbZ~|~\epsilon\\
Zb \rightarrow bZ \\
ZY \rightarrow Yc \\
Y \rightarrow \epsilon
\end{cases}
\]
\end{exercice}

\begin{correction*}
La grammaire commence obligatoirement par réécrire $S$ en $XY$. Le $X$ peut ensuite créer des copies de lui-même, une par une, en ajoutant $a$ à gauche et $bZ$ à droite, jusqu'à disparaître. L'application des deux premières règles a donc toujours la forme suivante :

\[
S \rightarrow_1 XY \rightarrow_2^n a^nX(bZ)^nY \rightarrow_3 a^n(bZ)^nY
\]

La règle 4 permet ensuite de faire rouler les $b$ vers les $a$ et les $Z$ vers le $Y$ : 

\[
a^n(bZ)^nY \rightarrow_4^* a^nb^nZ^nY
\]

Enfin, le $Y$ roule sur les $Z$, les transformant au passage en $c$, et peut disparaître une fois qu'il a fini son travail :


\[
a^n(bZ)^nY \rightarrow_5^* a^nb^nYc^n \rightarrow_6 a^nb^nc^n
\]

Notez qu'on peut faire disparaître le $Y$ avant qu'il ait fini de transformer les $Z$ en $c$. Mais dans ce cas, on ne pourrap as obtenir un mot composé uniquement de symboles terminaux, il s'agit donc d'une impasse sans conséquence.
\end{correction*}

\section{Hierarchie de Chomsky}


\subsection{Grammaires de type 3, ou régulières}

\subsubsection{Grammaires linéaires à gauche et à droite}


\begin{exercice}
Quel est le langage engendré par la grammaire suivante ?

\[
\begin{cases}
S \rightarrow Sa~|~Ta~|~a \\
T \rightarrow Tb~|~b
\end{cases}
\]

\end{exercice}

\begin{correction*}

L'axiome $S$ permet d'abord de générer autant de $a$ que désiré sur sa droite. On a ensuite le choix de s'arrêter avec un dernier $a$, produisant donc un mot de $a^+$, ou de partir en $T$ avec également un dernier $a$, et d'y générer un nombre non-nul de $b$. Le langage engendré par la grammaire est donc $a^+ + b^+a^+$, qui peut se factoriser en $b^*a^+$.

\end{correction*}


\begin{exercice}(**) Donner, en suivant l'intuition de la preuve ci-dessus, une grammaire linéaire à droite qui engendre le même langage que la grammaire 

\[
\begin{cases}
S \rightarrow Sa~|~Ta~|~a \\
T \rightarrow Tb~|~b
\end{cases}
\]

\end{exercice}

\begin{correction*}
Une dérivation dans la grammaire termine forcément par $S \rightarrow a$ ou $T \rightarrow b$. On a donc, pour la version droite de la grammaire, de commencer par écrire un $a$ suivi d'un $S$, ou un $b$ suivi d'un $T$. Soit $A$ l'axiome de la nouvelle grammaire, on a déjà les deux règles qui suivent :

\[
\begin{cases}
A \rightarrow aS~|~bT
\end{cases}
\]

La question qui suit est "comment peut-on créer un $S$ dans la grammaire originale ?". Une première possibilité est via la règle $\textcolor{red}{S} \rightarrow \textcolor{blue}{S}a$, qu'on inverse en 

\[
\begin{cases}
\textcolor{blue}{S} \rightarrow a\textcolor{red}{S}
\end{cases}
\]

De plus, $S$ étant l'axiome, il peut apparaître \textit{ex nihilo}. Il devient donc la fin des dérivations dans la nouvelle grammaire : 

\[
\begin{cases}
S \rightarrow \epsilon
\end{cases}
\]

La deuxième question est "comment peut-on créer un $T$ dans la grammaire originale ?" Les deux réponses sont $S \rightarrow Ta$ et $T \rightarrow Tb$, qu'on inverse en 


\[
\begin{cases}
T \rightarrow aS~|~bT
\end{cases}
\]

Au final, notre grammaire linéaire à droite est 

\[
\begin{cases}
A \rightarrow aS~|~bT \\
S \rightarrow aS~|~\epsilon \\
T \rightarrow aS~|~bT
\end{cases}
\]

où $A$ est l'axiome. On peut vérifier assez facilement que cette grammaire reconnaît également $b^*a^+$.

\end{correction*}


\subsubsection{Une extension du théorème de Kleene}
On a vu dans le chapitre \ref{hierarchie} que les langages pouvant être décrits par des expressions rationnelles et ceux définissables par automate étaient les mêmes. Ce résultat peut s'étendre aux grammaires de type 3.

\begin{exercice}
Traduire en automate la grammaire 
\[
\begin{cases}
S \rightarrow aS~|~bB~|~a~|~\epsilon \\
A \rightarrow aS~|~\epsilon \\
B \rightarrow a
\end{cases}
\]

\end{exercice}

\begin{correction*}
En appliquant l'algorithme donné en cours, on obtient 


\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.9cm,
                    semithick]
  \tikzstyle{every state}=[fill=white,text=black]
  \tikzstyle{place}=[rectangle,draw=black,fill=white, minimum size=7mm]


  \node[initial,state,accepting] (S)                    {$S$};
  \node[state,accepting] (A)      [right of=S]                {$A$};  
  \node[state] (B)      [right of=A]                {$B$};
  \node[accepting,state] (T)      [above of=A]                {$T$};
  
  \path %(I) edge[loop above]              node {$a,b$} (I)
(S) edge      [loop above]        node {$a$} (S)
(S) edge      [bend right]        node {$b$} (B)
(S) edge      []        node {$a$} (T)
(A) edge      [loop above]        node {$a$} (A)
(B) edge      []        node {$a$} (T)
;
\end{tikzpicture}
\end{figure}

\end{correction*}

\begin{exercice}
Traduire en grammaire l'automate 


\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.9cm,
                    semithick]
  \tikzstyle{every state}=[fill=white,text=black]
  \tikzstyle{place}=[rectangle,draw=black,fill=white, minimum size=7mm]


  \node[initial,state] (0)                    {$A$};
  \node[state] (1)      [right of=0]                {$B$};  
  \node[state] (2)      [right of=1]                {$C$};
  \node[accepting,state] (3)      [right of=2]                {$D$};
  \node[accepting,state] (4)      [right of=3]                {$E$};
  \node[accepting,state] (5)      [below of=4]                {$F$};

  \path %(I) edge[loop above]              node {$a,b$} (I)
(0) edge      [loop above]        node {$b$} (0)
(0) edge      []        node {$a$} (1)
(1) edge      [loop above]        node {$a$} (1)
(1) edge      []        node {$b$} (2)
(2) edge      [bend left]        node {$b$} (0)
(2) edge      []        node {$a$} (3)
(3) edge      [loop above]        node {$a$} (3)
(3) edge      []        node {$b$} (4)
(4) edge      [bend left]        node {$a$} (3)
(4) edge      []        node {$b$} (5)
(5) edge      [loop right]        node {$b$} (3)
(5) edge      []        node {$a$} (3);
\end{tikzpicture}
\end{figure}

\end{exercice}

\begin{correction*}
En appliquant l'algorithme donné en cours, on obtient 

\[
\begin{cases}
A \rightarrow bA~|~aB \\
B \rightarrow aB~|~bC \\
C \rightarrow aD~|~bA \\
D \rightarrow aD~|~bE~|~\epsilon \\
E \rightarrow aD~|~bF~|~\epsilon \\
F \rightarrow aD~|~bF~|~\epsilon
\end{cases}
\]

où $A$ est l'axiome.

Notez qu'on pourrait aussi ajouter par exemple une règle $C \rightarrow a$, mais ça serait redondant avec $C \rightarrow aD$ et $D \rightarrow \epsilon$. On pourrait également enlever toutes les $\epsilon$-productions et les remplacer par $C \rightarrow a$, $D \rightarrow a$ etc, mais la grammaire nous semble ainsi moins lisible.

\end{correction*}


\subsection{Grammaires de type 2, ou non contextuelles}


\subsection{Grammaires de type 1, ou contextuelles}


\begin{exercice}
Montrer que la grammaire suivante engendre bien le langage \newline $\{a^nb^nc^n ~|~ n \in \mathbb{N}\}$.

\[
\begin{cases}
S' \rightarrow S~|~\epsilon\\
S \rightarrow aSBC~|~aBC \\
CB \rightarrow HB\\
HB \rightarrow HC \\
HC \rightarrow BC \\
aB \rightarrow ab \\
bB \rightarrow bb \\
bC \rightarrow bc \\
cC \rightarrow cc 
\end{cases}
\]

\end{exercice}


\begin{correction*}

L'axiome $S'$ se réécrit en $S$ ou $\epsilon$, ce dernier étant donc inclus comme un cas particulier. $S$ peut écrire des $a$ à sa gauche et des $BC$ à sa droite via la règle 3, jusqu'à s'arrêter en produisant une dernière portée avec la règle 4. Toute dérivation passant par $S$ commence donc par une série de réécritures de la forme 

\[
S' \rightarrow_1 S \rightarrow_3^n a^nS(BC)^n \rightarrow_4 a^{n+1}(BC)^{n+1}
\]

Les règles 

\[
\begin{cases}
CB \rightarrow HB\\
HB \rightarrow HC \\
HC \rightarrow BC
\end{cases}
\]

permettent de simuler, dans le cadre des grammaires contextuelles, la règle $CB \rightarrow BC$. On peut donc faire passer les $B$ à gauche des $C$ (et inversement) :

\[
a^{n+1}(BC)^{n+1} \rightarrow_{5, 6, 7}^* a^{n+1}B^{n+1}C^{n+1}
\]

Les quatre dernières règles permettent enfin de transformer les $B$ en $b$ et les $C$ en $c$, en allant de la gauche vers la droite :

\[
a^{n+1}B^{n+1}C^{n+1} \rightarrow_8 a^{n+1}bB^{n}C^{n+1} \rightarrow_9^n a^{n+1}b^{n+1}C^{n+1} \rightarrow_{10} a^{n+1}b^{n+1}cC^{n} \rightarrow_{11}^n a^{n+1}b^{n+1}c^{n+1}
\]


\end{correction*}


\subsection{Grammaires de type 0, ou générales}

\section{Utilisation de grammaires algébriques}

\subsection{Arbres de dérivation}

\begin{exercice}
Quel est le langage engendré par la grammaire suivante ? 

\[
\begin{cases}
S \rightarrow (S)S ~|~ \epsilon
\end{cases}
\]
\end{exercice}

\begin{correction*}
Cette grammaire engendre l'ensemble des "bons parenthésages", comme $((())())(()())$.
\end{correction*}

\begin{exercice}
Donner un arbre de dérivation du mot abbbaaba dans la grammaire 

\[
\begin{cases}
S \rightarrow LaaR \\
L \rightarrow Lb \\
L \rightarrow ab \\
R \rightarrow bR \\
R \rightarrow ba
\end{cases}
\]

\end{exercice}

\begin{correction*}
On a l'arbre suivant :

\begin{figure}[H]
\center
\Tree[.S [.L [.L [.L a b ] b ] b ] a a [.R b a ] ]
\end{figure}

\end{correction*}

\subsection{Transformation de grammaires algébriques}

\subsubsection{Elimination d'$\epsilon$-productions}


\begin{exercice}
Eliminer les $\epsilon$-production de la grammaire suivante :

\[
\begin{cases}
S \rightarrow ABC~|~BD \\
A \rightarrow BC~|~ab \\
B \rightarrow CDC~|~\epsilon \\
C \rightarrow BAB~|~ABA \\
D \rightarrow abba~|~BB~|~BCBS
\end{cases}
\]
\end{exercice}

\begin{correction*}

$B$ se réécrit immédiatement en $\epsilon$, on met donc $B$ dans $K$.

$D$ se réécrit en $BB$, donc en $\epsilon$, on ajoute donc $D$ à $K$. De même, $S$ se réécrit en $BD$, qui est ajouté à $K$. On ne peut par contre ajouter ni $A$ ni $C$. On a donc $K = \{S,B,D\}$

En réécrivant les règle avec $K$, on obtient

\[
\begin{cases}
S \rightarrow ABC~|~BD\textcolor{blue}{~|~AC~|~B~|~D} \\
A \rightarrow BC~|~ab\textcolor{blue}{~|~C} \\
B \rightarrow CDC\text{\st{$~|~\epsilon$}}\textcolor{blue}{~|~CC} \\
C \rightarrow BAB~|~ABA\textcolor{blue}{~|~AB~|~BA~|~AA} \\
D \rightarrow abba~|~BB~|~BCBS\textcolor{blue}{~|~B~|~CBS~|~BCS~|~BCB~|~CS~|~CB~|~BC~|~C}
\end{cases}
\]

De plus, puisque $S \in K$, il faut ajouter la production d'$\epsilon$ comme cas spécifique :
\[
\begin{cases}
S' \rightarrow S~|~\epsilon \\
S \rightarrow ABC~|~BD\textcolor{blue}{~|~AC~|~B~|~D} \\
A \rightarrow BC~|~ab\textcolor{blue}{~|~C} \\
B \rightarrow CDC\text{\st{$~|~\epsilon$}}\textcolor{blue}{~|~CC} \\
C \rightarrow BAB~|~ABA\textcolor{blue}{~|~AB~|~BA~|~AA} \\
D \rightarrow abba~|~BB~|~BCBS\textcolor{blue}{~|~B~|~CBS~|~BCS~|~BCB~|~CS~|~CB~|~BC~|~C}
\end{cases}
\]

où $S'$ est l'axiome.

\end{correction*}

\subsubsection{Elimination de cycles}


\begin{exercice}
Eliminer les cycles de la grammaire 

\[
\begin{cases}
S \rightarrow AB~|~C\\
A \rightarrow C \\
B \rightarrow \epsilon \\
C \rightarrow S~|~a 
\end{cases}
\]
\end{exercice}

\begin{correction*}

Pour ça, il faut d'abord éliminer les $\epsilon$-productions de la grammaire. Puisque $K = \{B\}$, on obtient

\[
\begin{cases}
S \rightarrow AB~|~C~|~A\\
A \rightarrow C \\
C \rightarrow S~|~a 
\end{cases}
\]

On calcule que $S$, $A$ et $C$ peuvent tous se réécrire en $S$, $A$ et $C$. On transforme donc la grammaire en 

\[
\begin{cases}
S \rightarrow AB\text{\st{$~|~C~|~A$}}\textcolor{blue}{~|~a}\\
A \rightarrow \text{\st{$C$}}\textcolor{blue}{~|~AB~|~a}\\
C \rightarrow \text{\st{$S~|~$}}a\textcolor{blue}{~|~AB} 
\end{cases}
\]

On remarquera que cette grammaire est en fait un peu nulle.

\end{correction*}

\subsubsection{Elimination de symboles inutiles}

\begin{exercice}
Eliminer les symboles inutiles de la grammaire suivante :

\[
\begin{cases}
S \rightarrow BAB~|~DB \\
A \rightarrow B~|~a \\
B \rightarrow bB~|~Ba~|~DAD \\
C \rightarrow c~|~AB~|~AA \\
D \rightarrow Da~|~B~|~aSb
\end{cases}
\]

\end{exercice}

\begin{correction*}

On commence par exemple avec les symboles (in)accessibles. $S$ étant l'axiome, il est évidemment accessible. De plus, il peut immédiatement créer des $A$, $B$ et $D$, qui sont donc également accessibles. $A$ ne produit que $B$, $B$ ne produit que $B$ et $D$, et $D$ que des $S$, $B$ et $D$, on n'a donc rien à ajouter. Puisque $\{S,A,B,D\}$ est l'ensemble des symboles accessibles, l'ensemble des inaccessibles est $\{C\}$.

Notez qu'on aurait pu classer $C$ comme inaccessible plus rapidement en se rendant compte qu'il n'apparaît jamais à droite, mais on n'aurait pas été sûr qu'il composait l'intégralité des symboles inaccessibles.

Notre grammaire est donc maintenant :

\[
\begin{cases}
S \rightarrow BAB~|~DB \\
A \rightarrow B~|~a \\
B \rightarrow bB~|~Ba~|~DAD \\
D \rightarrow Da~|~B~|~aSb
\end{cases}
\]

On cherche ensuite les symboles avec production. Le seul non-terminal à pouvoir produire immédiatement un "vrai mot" est $A$, qui se réécrit en $a$. Cet ajout ne nous permet cependant pas de trouver un autre symbole productif, puisque aucune règle ne produit un mot composé uniquement de terminaux et/ou de $A$.

Puisque le seul symbole avec production est $\{A\}$, les symboles sans production sont $\{S,B,D\}$. Notre grammaire peut donc se simplifier en 


\[
\begin{cases}
A \rightarrow B~|~a 
\end{cases}
\]

Notre qu'en réappliquant l'élimination de symboles inaccessibles, on enlèverait le $A$, montrant que la grammaire originale engendrait en fait le langage vide.

\end{correction*}

